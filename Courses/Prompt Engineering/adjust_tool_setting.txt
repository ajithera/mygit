Temperature
top-p
top-k

These are all filtering mechanism of tokens.
Before this understand the tokenization concepts from tokenization.txt

Note:-----------------------------------------------------------------
These settings are not configureable for public access llms. 
These are configureable in API's like creating new llm application.
we can define those properties according to the need of application.

Gemini JSON snippet for config:
```
"generationConfig": {
  "temperature": 0.9,
  "topK": 1,
  "topP": 1,
}
```
---------------------------------------------------------------------
Temperature:
Temperature controls the creativeness of the output.
Low temperature (0.1-0.4): Less Creative (Technical work, document generation, formal communication)
Medium temperature (0.5-0.7): Moderate Creative. (General Chatbots, Balanced content creation)
More temperature (0.8 - 1.2+): More Creative. (Brain stroming, Creative Art, Marketting slogans)


Example:
Prompt: "Generate a slogan for a new brand of coffee called 'Aura'."

Low Temperature (e.g., 0.1 - 0.3) Response:
LLM's Thought Process: 
"The most common words associated with coffee are 'morning,' 'start,' 'best,' 'taste,' and 'aroma.' 
I will stick to these."
Expected Output:
1. "Aura Coffee: The best start to your morning."
2. "Aura: Awaken your senses."
3. "Taste the rich aroma of Aura Coffee."

These slogans are safe and professional and highly predictable. 
It is useful for brand to communicate directly with simple message.

High Temperature (e.g., 0.9 - 1.2) Response:
LLM's Thought Process: "Let's explore less obvious concepts. 
What else is an 'aura'? It relates to light, energy, feeling, and spirit. 
Let's connect coffee to those abstract ideas."

Expected Output:
1. "Aura Coffee: Brew your inner light."
2. "Aura: More than coffee, it's a feeling."
3. "Sip the spectrum. Taste the Aura."

These slogans are more creative, poetic, and memorable. 
They create an emotional connection rather than a purely functional one. 
However, there's a higher risk of them being too abstract or nonsensical. 
This is perfect for a modern, boutique brand targeting a niche audience.

Another example with code development:

Prompt: "Write a Python code comment explaining this function: def calculate_average(numbers):"
LLM's thought process: "This is a technical task. 
I must be precise, clear, and follow standard documentation practices. 
I'll describe exactly what the function does."

Low temperature Output:
```
# Calculates the arithmetic mean of a list of numbers.
# Args:
#   numbers: A list of numerical values.
# Returns:
#   The average of the numbers in the list.
def calculate_average(numbers):
```

This is professional response that developer would want.
Low temperature is right choice for technical work.

High temperature:
Thought Process: "Let's explain this in a more 'human' or creative way. 
What's the purpose of an average? It's about finding a central point."
Expected output:
```
# Ever wonder what the 'middle ground' for a bunch of numbers is?
# This little function finds that sweet spot for you. It takes all your
# numbers, adds them up, and then shares the love equally among them.
def calculate_average(numbers):
```
This comment is informal, conversational, and uses metaphors ("sweet spot," "shares the love"). 
While it might be charming in a personal project or a tutorial for beginners, 
it is unprofessional and distracting in a production codebase. 
It introduces the risk of being unclear and is generally not suitable for technical tasks.


Top-K:
It is also the filtering the tokens.
If the likely tokens for next word is 50000 and top-k value is 50 then it will pick from top 50 tokens with hight probability.
All other words will be ignored.
Once the pool is selected, the probabilities are reset for next top-p filteration.

Smaller Top-K value : More focused and less surprising.
Large Top-K value: More diverse and creative output.

Top-P:
It is also another filtering method of tokens.
From the top-K tokens it again filter the tokens based on cumulative probability.
If top-p = 0.9, then it will add probability of tokens from top order. 
Once the cumalative >= 0.9 it ignore all other tokens.

top-p = 0.9

token1 = 0.4 (cumalative = 0.4)
token2 = 0.35 (cumalative = 0.75)
token3 = 0.3 (cumalative = 1.05) Here it crosses the top-p value.
token4 = 0.25 

From the above example llm picks the word from top 3 tokens. Ignore the rest.


Execution tree:
First temperature condition is applied.
From that top-k is applied.
From that top-p is applied.