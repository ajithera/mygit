input prompt --> prompt to tokens(by tokenizer) --> Embeddings assigned to tokens by embedding matrix

User Prompt â†’ Tokenizer â†’ Tokens â†’ Embedding Matrix Lookup
â†’ Embeddings â†’ Transformer Layers (attention + weights)
â†’ Probability distribution over next tokens
â†’ Sampling (temperature, top-k, top-p) â†’ Output token
â†’ Repeat until full answer is generated


1.User prompt â†’ Tokenizer
"Explain solar system" â†’ [Explain] [solar] [system].

2.Tokens â†’ Embeddings
Each token looked up in the embedding matrix (pre-trained during training).
So now the model has vectors that represent the meaning of those words.

3.Embeddings â†’ Transformer layers
This is the engine.
The embeddings go through multiple transformer blocks (attention + feedforward).
The final layer outputs a probability distribution over the entire vocabulary.
Example:
â€œTheâ€ â†’ 0.65
â€œItâ€ â†’ 0.20
â€œSolarâ€ â†’ 0.08
â€¦ thousands of other tokens with smaller probabilities.
ğŸ‘‰ So the transformer assigns the probabilities, not the embedding matrix.
The embeddings just give the input meaning.

4.Sampling from probabilities
Now you apply temperature, top-k, top-p to this probability distribution.
Example:
With low temperature â†’ always pick â€œTheâ€ (most probable).
With higher temperature â†’ maybe â€œItâ€ or â€œAâ€ could be picked instead.

5.Output token is chosen â†’ added to context
Then step repeats â†’ predict next token.